<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="computer vision,">










<meta name="description" content="Object Detection[TOC]  1    Random Decision Forests​    A finite set of randomly generated decision trees forms a forest, called a random decision forest (RDF). 1.1    Entropy and Information Gain1.1.">
<meta name="keywords" content="computer vision">
<meta property="og:type" content="article">
<meta property="og:title" content="Object Detection">
<meta property="og:url" content="http://yoursite.com/2019/05/04/Object-Detection/index.html">
<meta property="og:site_name" content="Wingardium Leviosa">
<meta property="og:description" content="Object Detection[TOC]  1    Random Decision Forests​    A finite set of randomly generated decision trees forms a forest, called a random decision forest (RDF). 1.1    Entropy and Information Gain1.1.">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556790753505.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556873478232.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556880072965.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556883033937.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556884175680.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556906177033.png">
<meta property="og:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556906625408.png">
<meta property="og:updated_time" content="2019-05-04T12:08:34.209Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Object Detection">
<meta name="twitter:description" content="Object Detection[TOC]  1    Random Decision Forests​    A finite set of randomly generated decision trees forms a forest, called a random decision forest (RDF). 1.1    Entropy and Information Gain1.1.">
<meta name="twitter:image" content="http://yoursite.com/2019/05/04/Object-Detection/C:/Users/new/AppData/Roaming/Typora/typora-user-images/1556790753505.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/05/04/Object-Detection/">





  <title>Object Detection | Wingardium Leviosa</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Wingardium Leviosa</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/04/Object-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZhuangZi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Wingardium Leviosa">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Object Detection</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-04T20:07:14+08:00">
                2019-05-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><p>[TOC]</p>
<hr>
<h2 id="1-Random-Decision-Forests"><a href="#1-Random-Decision-Forests" class="headerlink" title="1    Random Decision Forests"></a>1    Random Decision Forests</h2><p>​    A finite set of randomly generated decision trees forms a <em>forest</em>, called a <em>random decision forest</em> (RDF).</p>
<h3 id="1-1-Entropy-and-Information-Gain"><a href="#1-1-Entropy-and-Information-Gain" class="headerlink" title="1.1    Entropy and Information Gain"></a>1.1    Entropy and Information Gain</h3><h4 id="1-1-1-Entropy"><a href="#1-1-1-Entropy" class="headerlink" title="1.1.1    Entropy"></a>1.1.1    Entropy</h4><p>$S = {x_1,…, x_n}$:    A finite  alphabet</p>
<p>$X$:    A discrete random variable taking values from $S$ </p>
<p>$p_i = P(X=x_i),\quad i = 1,…,n$</p>
<h5 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h5><p>​    The information content of $X$ is<br>$$<br>I(X) = -\log P(X)<br>$$</p>
<ul>
<li><p>$I(X)$ is also a random variable, where $I(X=a_i) = -\log p_i$</p>
</li>
<li><p>(Monotonically decreasing)</p>
</li>
<li><p>(Additivity of independent events)<br>$$<br>\begin{align<em>}<br>I_{X, Y}(X=x, Y=y) &amp;= -\log P_{X, Y}(x, y) = -\log P_X(x)P_Y(y)\<br>&amp;= -\log P_X(x) - \log P_Y(y)\<br>&amp;= I_X(X=x) + I_Y(Y=y)<br>\end{align</em>}<br>$$</p>
</li>
</ul>
<h5 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h5><p>​    The <em>entropy</em> of $X$ is the expected information carried by $X$. That is<br>$$<br>H(X) = E[I(X)] = E[-\log P(X)] = -\sum_{i=1}^n p_i\log p_i<br>$$</p>
<ul>
<li>If $X\sim Binomial(1, \ 0.5)$, then $H(X) = 1$</li>
<li>Maximum entropy is given if all $p_i$ is equal</li>
</ul>
<h5 id="Conditional-Entropy"><a href="#Conditional-Entropy" class="headerlink" title="Conditional Entropy"></a>Conditional Entropy</h5><p>​    Suppose X is a r.v. over the set $S$, $Y$ is a r.v. over the set $T$, then<br>$$<br>H(Y|X) = -\sum_{x\in S}\sum_{y\in T}P(x, y)\log P(y|x)<br>$$</p>
<h4 id="1-1-2-Information-Gain-信息增益"><a href="#1-1-2-Information-Gain-信息增益" class="headerlink" title="1.1.2    Information Gain (信息增益)"></a>1.1.2    Information Gain (信息增益)</h4><p>​    The information gain (KL divergence / Relative entropy)<br>$$<br>G(Y|X) = H(Y) - H(Y|X)<br>$$<br>is the number of bits saved on average.</p>
<h3 id="1-2-Sketch-of-Random-Forest"><a href="#1-2-Sketch-of-Random-Forest" class="headerlink" title="1.2    Sketch of Random Forest"></a>1.2    Sketch of Random Forest</h3><h4 id="1-2-1-Decision-tree-learning"><a href="#1-2-1-Decision-tree-learning" class="headerlink" title="1.2.1    Decision tree learning"></a>1.2.1    Decision tree learning</h4><p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556790753505.png" alt="1556790753505"></p>
<h4 id="1-2-2-Bagging-Bootstrap-aggregating"><a href="#1-2-2-Bagging-Bootstrap-aggregating" class="headerlink" title="1.2.2    Bagging (Bootstrap aggregating)"></a>1.2.2    Bagging (Bootstrap aggregating)</h4><p>​    Given a training set $X = x_1,…,x_n$ with responses $Y = y_1,…,y_n$, bagging repeatedly (<em>B</em> times) selects a <a href="https://en.wikipedia.org/wiki/Sampling_(statistics" target="_blank" rel="noopener">random sample with replacement</a>#Replacement_of_selected_units) of the training set and fits trees to these samples:</p>
<p>​    For $b = 1,…,B$:</p>
<ol>
<li>Sample $n$ training data from $X, Y$, called $X_b, Y_b$.<ol start="2">
<li>Train a classification or regression tree $f_b$ on $X_b, Y_b$.</li>
</ol>
</li>
</ol>
<p>​    Predict:<br>$$<br>\hat f = \frac{1}{B}\sum_{b=1}^Bf_b(x’)<br>$$</p>
<h4 id="1-2-3-Random-forest"><a href="#1-2-3-Random-forest" class="headerlink" title="1.2.3    Random forest"></a>1.2.3    Random forest</h4><ul>
<li><p>Random forest = Decision tree + Bagging</p>
</li>
<li><p>There are two bagging factors in random forest training: data &amp; features.</p>
</li>
<li><p>Hyperparameter: the number of selected features  $m$:</p>
<ul>
<li><p>if $m$ is too small, the classification ability of each tree gets weaker.</p>
</li>
<li><p>if m is too big, the correlation between trees gets stronger, leading to higher variance. </p>
</li>
</ul>
</li>
</ul>
<h3 id="1-3-Applying-a-Forest"><a href="#1-3-Applying-a-Forest" class="headerlink" title="1.3    Applying a Forest"></a>1.3    Applying a Forest</h3><p>Given a trained random forest that consists of $w$ trees $T_1,…,T_w$. </p>
<p>Given image data and rectangular window $W$. </p>
<p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556873478232.png" alt="1556873478232"></p>
<h3 id="1-4-Training-a-Forest"><a href="#1-4-Training-a-Forest" class="headerlink" title="1.4    Training a Forest"></a>1.4    Training a Forest</h3><p>​    We apply <strong>supervised learning</strong> for generating a random decision forest.</p>
<ul>
<li>$S$:    a set pf samples.</li>
<li>$P = [I, d]$:    a sample, means an image window $I$ with a class number $d$  ($0\leq d\leq k$).</li>
<li>$S = S_0 \cup \bigcup_{d=1}^kS_d$.</li>
</ul>
<h4 id="Split-function"><a href="#Split-function" class="headerlink" title="Split function"></a>Split function</h4><p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556880072965.png" alt="1556880072965"></p>
<p>​    The applied rule at a <strong>split node</strong> $v$ is also called a split function. </p>
<p>​    A unary split function $h_\phi$ decides which node (left or right) comes next:<br>$$<br>\begin{align}<br>S_L(\phi) &amp;= {I\in S_v: h_\phi(I) = 0}\<br>S_R(\phi) &amp;= {I\in S_v: h_\phi(I) = 1}<br>\end{align}<br>$$</p>
<h4 id="Outline-for-growing-a-tree"><a href="#Outline-for-growing-a-tree" class="headerlink" title="Outline for growing a tree"></a>Outline for growing a tree</h4><p>​    During training, a randomly selected subset<br>$$<br>S_i = S_{i,0}\cup \bigcup_{d=1}^kS_{i, d}\subset S<br>$$<br>is employed for growing a tree $T_i$($1\leq i\leq w$).</p>
<h5 id="Tree-Growing-Procedure"><a href="#Tree-Growing-Procedure" class="headerlink" title="Tree-Growing Procedure"></a>Tree-Growing Procedure</h5><p>​    [Algorithm]</p>
<ul>
<li>Start with a single root node, being the only <strong><em>active node</em></strong></li>
<li>Recursively do:<ul>
<li>Decide for each <strong><em>active node</em></strong> whether it should turn into:<ul>
<li>a <strong><em>split node</em></strong> (with selecting a suitable <em>split function</em> $h_{\phi}$) </li>
<li>or a <strong><em>leaf node</em></strong> (defined by a <em>stop criterion</em>)</li>
</ul>
</li>
<li>All <strong><em>active node</em></strong> become <strong><em>passive node</em></strong>, and newly created split nodes become <strong><em>active node</em></strong></li>
</ul>
</li>
</ul>
<h5 id="Leaf-Definition-Procedure"><a href="#Leaf-Definition-Procedure" class="headerlink" title="Leaf-Definition Procedure"></a>Leaf-Definition Procedure</h5><p>​    We not only assign one class to one leaf node. Instead, we need to analyze the distribution of classes represented.</p>
<p>​    At a leaf node $L$, we estimate the classification probability $p(c=0|L)$ and the class probabilities<br>$$<br>p(c\neq 0, d|L) = [1-p(c=0|L)]\cdot p(d|L)<br>$$<br>for $d\in {1,…,p}$.</p>
<h5 id="Compensate-for-Sample-Bias"><a href="#Compensate-for-Sample-Bias" class="headerlink" title="Compensate for Sample Bias"></a>Compensate for Sample Bias</h5><p>​    The classification probabilities $p(c|L)$ and $p(d|L)$ are estimated based on the number of samples from sets $S_{i,0}$ or $S_{i, d}$, $1\leq d\leq p$, which causes a bias.</p>
<p>​    At the node $L$, the set of all training samples arriving at $L$ is<br>$$<br>S_i^L = S_{i,0}^L\cup\bigcup_{d=1}^pS_{i,d}^L\subset S_iWithout compensation,<br>$$<br>​    Without compensation,<br>$$<br>\begin{align}<br>p(c=0|L) &amp;= \frac{|S_{i,0}^L|}{S_i^L}\<br>p(d|L) &amp;= \frac{|S_{i,d}^L|}{|\bigcup_{d=1}^pS_{i,d}^L|}<br>\end{align}<br>$$<br>​    The estimates assume that ratio<br>$$<br>r_{i,0} = \frac{|S_{i,0}|}{|S_i|}\quad\text{and}\quad r_{i,d} = \frac{|S_{i,d}|}{|S_i|}<br>$$<br>are about the same as the ratio<br>$$<br>r_0 = \frac{|S_0|}{|S|}\quad\text{and}\quad r_d = \frac{|S_d|}{|S|}<br>$$<br>​    To compensate for sample bias:<br>$$<br>\begin{align}<br>p(c=0|L) &amp;= \frac{|S_{i,0}^L|}{S_i^L}\cdot\frac{r_0}{r_{i,0}}\<br>p(d|L) &amp;= \frac{|S_{i,d}^L|}{|\bigcup_{d=1}^pS_{i,d}^L|}\cdot\frac{r_d}{r_{i,d}}<br>\end{align}<br>$$</p>
<h5 id="Features-and-Split-Functions"><a href="#Features-and-Split-Functions" class="headerlink" title="Features and Split Functions"></a>Features and Split Functions</h5><p>​    The split function should be design for maximizing the <em>information gain</em>.</p>
<p>​    A simple option of split function:<br>$$<br>h_\phi(I) = \begin{cases}<br>0, \quad\text{if}\ I_{loc}(p) - I_{loc}(q)\geq\tau\<br>1, \quad\text{otherwise}<br>\end{cases}<br>$$<br>where $\phi = {p, q, \tau}$.</p>
<h5 id="Learn-a-Split-Node"><a href="#Learn-a-Split-Node" class="headerlink" title="Learn a Split Node"></a>Learn a Split Node</h5><ul>
<li>Suitable parameters $\phi$ are learned from samples.</li>
<li>The parameters $\phi$ are selected randomly.</li>
</ul>
<h5 id="Stop-Criterion"><a href="#Stop-Criterion" class="headerlink" title="Stop Criterion"></a>Stop Criterion</h5><p>​    Examples for a stop criterion:</p>
<ul>
<li>the depth of the node in the tree (i.e. the depth is limited by a maximum value) </li>
<li>the number of samples reaching this node is below a threshold</li>
</ul>
<h5 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h5><p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556883033937.png" alt="1556883033937"></p>
<h3 id="1-5-Hough-Forests-霍夫森林"><a href="#1-5-Hough-Forests-霍夫森林" class="headerlink" title="1.5    Hough Forests (霍夫森林)"></a>1.5    Hough Forests (霍夫森林)</h3><h4 id="An-outline-of-HF"><a href="#An-outline-of-HF" class="headerlink" title="An outline of HF:"></a>An outline of HF:</h4><ul>
<li>Allow us to use only small patches when applying the classifier</li>
<li>Combine localization and classification within one integrated process</li>
</ul>
<h4 id="Centroid-Parameterization"><a href="#Centroid-Parameterization" class="headerlink" title="Centroid Parameterization"></a>Centroid Parameterization</h4><p>​    We take the centroid of an object as a parametric description of an object:</p>
<p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556884175680.png" alt="1556884175680"></p>
<h4 id="Training-a-HF"><a href="#Training-a-HF" class="headerlink" title="Training a HF"></a>Training a HF</h4><p>​    Samples in the training set $S$ are now triples $P = [I, d, \textbf{a}]$ instead $[I, d]$.</p>
<ul>
<li>$I$ is now a small patch only</li>
<li>$\textbf{a}$ is the distance vector between the patch and the centroid of the object of class $d$</li>
</ul>
<p>​    The optimization function for $\textbf{a}$:<br>$$<br>\min{\sum_{I_l}(a_i^l - \bar a^l)^2 + \sum_{l_r}(a_i^r-\bar a^r)^2}<br>$$<br>​    When splitting a tree node, choose the optimization criterion randomly.</p>
<hr>
<h2 id="2-Pedestrian-Detection"><a href="#2-Pedestrian-Detection" class="headerlink" title="2    Pedestrian Detection"></a>2    Pedestrian Detection</h2><h3 id="2-1-Outline"><a href="#2-1-Outline" class="headerlink" title="2.1    Outline"></a>2.1    Outline</h3><ul>
<li>describes a way how to localize object candidates ($i.e.$ rectangular RoIs to be tested in the RDF)</li>
<li>provides specific split functions</li>
<li>outlines a post-processing step.</li>
</ul>
<h3 id="2-2-Localizing-Regions-of-Interest-RoI"><a href="#2-2-Localizing-Regions-of-Interest-RoI" class="headerlink" title="2.2    Localizing Regions of Interest (RoI)"></a>2.2    Localizing Regions of Interest (RoI)</h3><p>​    To localize RoIs, we use disparity maps (视差图) produced by a stereo matcher (立体匹配器).</p>
<p>​    But if we use Hough Forests (HF) as the classifier,  we can choose random patches in image where pedestrians are likely to appear.</p>
<h3 id="2-3-Split-Function"><a href="#2-3-Split-Function" class="headerlink" title="2.3    Split Function"></a>2.3    Split Function</h3><p>​    HoG (方向梯度直方图)：</p>
<p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556906177033.png" alt="1556906177033"></p>
<p>​    Based on the HoG descriptors, a possible choice for split functions is<br>$$<br>h_\phi(I) = \begin{cases}<br>0,\quad\text{if}\ B(a,i)-B(b,j) &gt; \tau\<br>1,\quad\text{otherwise}<br>\end{cases}<br>$$<br>where $B(a,i)$ and $B(b,j)$ are the accumulated magnitude values in block $a$ and $b$, bin $i$ and $j$.</p>
<h3 id="2-4-Non-maximum-Suppression"><a href="#2-4-Non-maximum-Suppression" class="headerlink" title="2.4    Non-maximum Suppression"></a>2.4    Non-maximum Suppression</h3><p>​    Often, classification detects more than one window around a person. It is meaningful to merge them into one window.</p>
<p><img src="/2019/05/04/Object-Detection/C:/Users\new\AppData\Roaming\Typora\typora-user-images\1556906625408.png" alt="1556906625408"></p>
<p>​    Two approtches:</p>
<ul>
<li>Each positive window has the corresponding probability assigned by RDF. Choose the one with the highest probability. </li>
<li>Having window centers and probabilities, a mean-shift mode-seeking procedure could be applied to detect an object.</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/computer-vision/" rel="tag"># computer vision</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/25/PLANS-in-Statistics/" rel="next" title="PLANS in Statistics">
                <i class="fa fa-chevron-left"></i> PLANS in Statistics
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">ZhuangZi</p>
              <p class="site-description motion-element" itemprop="description">statistics</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-Detection"><span class="nav-number">1.</span> <span class="nav-text">Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Random-Decision-Forests"><span class="nav-number">1.1.</span> <span class="nav-text">1    Random Decision Forests</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Entropy-and-Information-Gain"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1    Entropy and Information Gain</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-Entropy"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1.1.1    Entropy</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Information"><span class="nav-number">1.1.1.1.1.</span> <span class="nav-text">Information</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Entropy"><span class="nav-number">1.1.1.1.2.</span> <span class="nav-text">Entropy</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Conditional-Entropy"><span class="nav-number">1.1.1.1.3.</span> <span class="nav-text">Conditional Entropy</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-Information-Gain-信息增益"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">1.1.2    Information Gain (信息增益)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Sketch-of-Random-Forest"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2    Sketch of Random Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-Decision-tree-learning"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.2.1    Decision tree learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-Bagging-Bootstrap-aggregating"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">1.2.2    Bagging (Bootstrap aggregating)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-3-Random-forest"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">1.2.3    Random forest</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Applying-a-Forest"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3    Applying a Forest</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Training-a-Forest"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4    Training a Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Split-function"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">Split function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Outline-for-growing-a-tree"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">Outline for growing a tree</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Tree-Growing-Procedure"><span class="nav-number">1.1.4.2.1.</span> <span class="nav-text">Tree-Growing Procedure</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Leaf-Definition-Procedure"><span class="nav-number">1.1.4.2.2.</span> <span class="nav-text">Leaf-Definition Procedure</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Compensate-for-Sample-Bias"><span class="nav-number">1.1.4.2.3.</span> <span class="nav-text">Compensate for Sample Bias</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Features-and-Split-Functions"><span class="nav-number">1.1.4.2.4.</span> <span class="nav-text">Features and Split Functions</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Learn-a-Split-Node"><span class="nav-number">1.1.4.2.5.</span> <span class="nav-text">Learn a Split Node</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Stop-Criterion"><span class="nav-number">1.1.4.2.6.</span> <span class="nav-text">Stop Criterion</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Algorithm"><span class="nav-number">1.1.4.2.7.</span> <span class="nav-text">Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Hough-Forests-霍夫森林"><span class="nav-number">1.1.5.</span> <span class="nav-text">1.5    Hough Forests (霍夫森林)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#An-outline-of-HF"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">An outline of HF:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Centroid-Parameterization"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">Centroid Parameterization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-a-HF"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">Training a HF</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Pedestrian-Detection"><span class="nav-number">1.2.</span> <span class="nav-text">2    Pedestrian Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Outline"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1    Outline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Localizing-Regions-of-Interest-RoI"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2    Localizing Regions of Interest (RoI)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Split-Function"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3    Split Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Non-maximum-Suppression"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4    Non-maximum Suppression</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZhuangZi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
